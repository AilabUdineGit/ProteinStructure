{
    "ROOT": "Protein/",
    "CORPUS": {
        "train": "data/Ntrain_data.txt",
        "test": "data/Ntest_data.txt"
    },
    "TOK": {
        "tok_class": "class BertWordPieceTokenizer",
        "out": "Protein/tokenizer_output_files/BertWordPieceTokenizer",
        "tokenizer_args": "Protein/tokenizer_output_files/BertWordPieceTokenizer/tokenizer_args.json",
        "fast_tokenizer_args": "Protein/tokenizer_output_files/BertWordPieceTokenizer/fast_tokenizer_args.json",
        "vocab_size": 5000,
        "min_frequency": 0,
        "lower": true
    },
    "MODEL": {
        "pretraining": {
            "block_size": 64,
            "mlm_prob": 0.15,
            "batch_size": 256,
            "gradient_accumulation_steps": 8,
            "epochs": 50,
            "save_limit": null,
            "save_steps": "2-epoch"
        },
        "name": "maxseq_64_bs_256x8_ep_50_nhidden_4",
        "corpus": "Protein/model_training_files",
        "out": "Protein/model_output_files",
        "hidden_size": 768,
        "max_pos_emb": 64,
        "num_attention_heads": 4,
        "num_hidden_layers": 4
    }
}