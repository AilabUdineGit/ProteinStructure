{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1587dc0f-f58a-498f-84a0-d89ad9384b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import cuda\n",
    "from tqdm import tqdm\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"t5-small\"\n",
    "FINETUNED_MODEL_NAME = \"t5_20_epochs\"\n",
    "EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VALID_BATCH_SIZE = 256\n",
    "SEQ_LEN=17\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(FINETUNED_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "tokenizer_r = AutoTokenizer.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d921eb4d-2d10-4c70-b20c-daa7fe9b3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batchify(l, batch_size):\n",
    "    i = 0\n",
    "    while i < len(l):\n",
    "        i += batch_size\n",
    "        yield l[i-batch_size:i]\n",
    "\n",
    "def to_tensor_dataset(data, tokenizer, kind):\n",
    "    \n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    outputs = tokenizer_r([\"Helyx\", \"Strand\", \"Coil\"], padding=\"longest\")\n",
    "    outputs = {i:v for i,v in enumerate(outputs.input_ids)}\n",
    "    \n",
    "    labels = data.target.apply(lambda x: outputs[x])#[outputs[row.target] for _,row in tqdm(data.iterrows(), total=len(data))]\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    print(labels.shape)\n",
    "        \n",
    "    batch_size = 200\n",
    "    \n",
    "    for sentences in tqdm(batchify(data.Seq, batch_size), total=len(data)//batch_size):\n",
    "        \n",
    "        tok_out = tokenizer(sentences.tolist(), add_special_tokens=False, padding=\"longest\")\n",
    "        tok_out = [\n",
    "            [x for x in seq if (x != 3) and (x != 0)]\n",
    "            for seq in tok_out.input_ids\n",
    "        ]\n",
    "        tok_out = [\n",
    "            seq + [tokenizer.pad_token_id]*(SEQ_LEN-len(seq))\n",
    "            for seq in tok_out\n",
    "        ]\n",
    "        \n",
    "        input_ids += tok_out\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    \n",
    "    return TensorDataset(input_ids, labels)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"train_dataset.pkl\"):\n",
    "\n",
    "    train_dataset = to_tensor_dataset(df, tokenizer, \"TRAIN\")\n",
    "    test_dataset = to_tensor_dataset(df_, tokenizer, \"TEST\")\n",
    "\n",
    "    with open(\"train_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_dataset, f)\n",
    "    with open(\"test_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a2c843-a042-458a-9655-b6c01e0d0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_dataset.pkl\", \"rb\") as f:\n",
    "#     train_dataset = pickle.load(f)\n",
    "with open(\"test_dataset.pkl\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c8c9df-b79e-42a9-b26d-f171a8c6eb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1636/1636 [01:07<00:00, 24.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "all_pred = []\n",
    "all_gold = []\n",
    "\n",
    "for input_ids, labels in tqdm(test_dataloader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            inputs=input_ids.to(device),\n",
    "            max_new_tokens=1\n",
    "        )\n",
    "    pred = tokenizer.batch_decode(out[:,-1])#out[:,-1].tolist()#tokenizer.batch_decode(out[:,-1])\n",
    "    gold = tokenizer.batch_decode(labels[:,0])#labels[:,-1].tolist()#tokenizer.batch_decode(labels[:,-1])\n",
    "    \n",
    "    all_pred += pred\n",
    "    all_gold += gold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b253242d-a73f-431d-ba48-9899997eca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold labels | {'Co', 'Strand', 'He'}\n",
      "pred labels | {'Co', 'Strand', 'He'}\n"
     ]
    }
   ],
   "source": [
    "print(\"gold labels |\", set(all_gold))\n",
    "print(\"pred labels |\", set(all_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edb465d0-d24f-4313-8d90-4342b1cb664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {\n",
    "    \"He\": 0,\n",
    "    \"Strand\": 1,\n",
    "    \"Co\": 2,\n",
    "}\n",
    "all_pred = [pred_dict[x] for x in all_pred]\n",
    "\n",
    "gold_dict = pred_dict\n",
    "all_gold = [gold_dict[x] for x in all_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1804065-25c6-4da6-b7b1-0f3d637bd4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75    202335\n",
      "           1       0.67      0.67      0.67    183014\n",
      "           2       0.61      0.56      0.58    105263\n",
      "\n",
      "    accuracy                           0.69    490612\n",
      "   macro avg       0.67      0.66      0.67    490612\n",
      "weighted avg       0.68      0.69      0.68    490612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(all_gold, all_pred, labels=list(set(all_gold))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aa7ae60-6be1-48f2-80c6-6e0d3ec6dc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490607</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490608</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490609</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490610</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490611</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490612 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        real  pred\n",
       "0          1     1\n",
       "1          1     1\n",
       "2          1     1\n",
       "3          1     1\n",
       "4          1     1\n",
       "...      ...   ...\n",
       "490607     1     0\n",
       "490608     1     0\n",
       "490609     1     1\n",
       "490610     1     1\n",
       "490611     1     1\n",
       "\n",
       "[490612 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_summary_test = pd.DataFrame({\"real\": all_gold, \"pred\": all_pred})\n",
    "pred_summary_test.to_pickle(FINETUNED_MODEL_NAME+\"/pred_summary_test.pickle\")\n",
    "pred_summary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f405d925-0212-4981-a399-e5070d9e5b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./checkpoint-221600.\n",
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5672821\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 443200\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 20\n",
      "  Continuing training from global step 221600\n",
      "  Will skip the first 20 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec01675e5d884b30b656c9e30e24d77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='221640' max='443200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [221640/443200 00:08 < 14:01:33, 4.39 it/s, Epoch 20.00/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import pickle\n",
    "import torch\n",
    "model = transformers.T5ForConditionalGeneration.from_pretrained(\"checkpoint-221600\")\n",
    "\n",
    "EPOCHS = 40\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VALID_BATCH_SIZE = 256\n",
    "SEQ_LEN=17\n",
    "\n",
    "with open(\"train_dataset.pkl\", \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "with open(\"test_dataset.pkl\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    # ------------------------------------------------------- [epochs and batch size]\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE*2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # ------------------------------------------------------- [hyperparams]\n",
    "    warmup_steps=100, \n",
    "    weight_decay=0.01,\n",
    "    # ------------------------------------------------------- [save and logging]\n",
    "    output_dir=\".\", \n",
    "    overwrite_output_dir = True,\n",
    "    do_eval = False,\n",
    "    logging_strategy=\"epoch\", # activate if interested\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit = None,\n",
    "    # -------------------------------------------------------\n",
    ")\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset,\n",
    "    data_collator = lambda data: {\n",
    "        'input_ids': torch.stack([f[0] for f in data]), \n",
    "        # 'attention_mask': torch.stack([f[1] for f in data]), \n",
    "        'labels': torch.stack([f[1] for f in data]),\n",
    "    },\n",
    "    # resume_from_checkpoint=True\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model(\"t5_20+20_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfe189-b247-4c88-8e4e-7a1c8bf64b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp_base",
   "language": "python",
   "name": "bp_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
